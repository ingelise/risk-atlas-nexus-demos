{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92b30a6-4a3c-4215-a0b8-3f61480ccba3",
   "metadata": {},
   "source": [
    "# Generating global explanations of LLM-as-a-Judge using GloVE algorithm\n",
    "\n",
    "This notebook shows how you might run the full pipeline and generate a global summary given a dataset and an LLM-as-a-Judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e8712c-f06a-4bad-970b-fdcf85c3aa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ingevejs/Documents/workspace/ingelise/risk-atlas-nexus-demos/risk-policy-distillation/.venv-risk-policy-distillation/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-02 11:16:29 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-02 11:16:30] INFO loader.py:156: Loading faiss.\n",
      "[2025-12-02 11:16:30] INFO loader.py:158: Successfully loaded faiss.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from risk_policy_distillation.datasets.prompt_response_dataset import (\n",
    "    PromptResponseDataset,\n",
    ")\n",
    "from risk_policy_distillation.datasets.abs_dataset import AbstractDataset\n",
    "from risk_policy_distillation.models.explainers.local_explainers.lime import LIME\n",
    "from risk_policy_distillation.models.explainers.local_explainers.shap_vals import SHAP\n",
    "from risk_policy_distillation.models.guardians.guardian import Guardian\n",
    "from risk_policy_distillation.pipeline.clusterer import Clusterer\n",
    "from risk_policy_distillation.pipeline.concept_extractor import Extractor\n",
    "from risk_policy_distillation.pipeline.pipeline import Pipeline\n",
    "\n",
    "# use AI atlas nexus for the inference tasks\n",
    "from ai_atlas_nexus.blocks.inference import (\n",
    "    InferenceEngine,\n",
    "    RITSInferenceEngine,\n",
    "    WMLInferenceEngine,\n",
    "    OllamaInferenceEngine,\n",
    "    VLLMInferenceEngine,\n",
    ")\n",
    "from ai_atlas_nexus.blocks.inference.params import (\n",
    "    InferenceEngineCredentials,\n",
    "    RITSInferenceEngineParams,\n",
    "    WMLInferenceEngineParams,\n",
    "    OllamaInferenceEngineParams,\n",
    "    VLLMInferenceEngineParams,\n",
    ")\n",
    "\n",
    "from ai_atlas_nexus.library import AIAtlasNexus\n",
    "\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f2b11f-d205-4f81-93bb-9ab3dd785db8",
   "metadata": {},
   "source": [
    "## Task\n",
    "Explain output from an LLM-as-a-Judge. \n",
    "\n",
    "## Create a dataset\n",
    "To explain the LLM-as-a-Judge, a dataset must be provided. [AbstractDataset](../../src/risk_policy_distillation/datasets/abs_dataset.py) class provides a wrapper for a dataframe you want to explain. You can use [PromptDataset](../../src/risk_policy_distillation/datasets/prompt_dataset.py) or [PromptResponseDataset](../../src/risk_policy_distillation/datasets/prompt_response_dataset.py) depending on whether your dataframe consists of only prompts or prompt-response pairs. You can also create a custom dataset by inheriting the Dataset class.\n",
    "\n",
    "### Setup dataset configuration\n",
    "In the cell below, an example of configuration with information on column name mapping is shown. \n",
    "\n",
    "A small sample from the dataset [PKU-Alignment/BeaverTails](https://github.com/PKU-Alignment/beavertails) is chosen to illustrate the example.  BeaverTails was developed to support research on safety alignment in large language models (LLMs), and consists of 300k+ human-labeled question-answering (QA) pairs, each associated with specific harm categories.  This sample and configuration are used to create a PromptResponseDataset.\n",
    "\n",
    "Additional parameters: \n",
    "\n",
    "_flip_labels_ indicates whether labels of the dataframe should be flipped in preprocessing step (e.g. for BeaverTails where labels indicate that the content is safe rather than harmful); \n",
    "\n",
    "_split_ indicates whether a train-val-test split needs to be performed during preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bc4d137-23e8-47da-afbd-88947bc07643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "ds = load_dataset(\"PKU-Alignment/BeaverTails\")\n",
    "dataframe = ds.data[\"330k_train\"].table.to_pandas()\n",
    "\n",
    "# sample for testing purposes\n",
    "dataframe = dataframe.sample(3)\n",
    "\n",
    "# provide some information in the data config\n",
    "data_config = {\n",
    "    \"dataset_name\": \"BeaverTails\",\n",
    "    \"index_col\": \"\",  # if there is no index column it will be automatically populated\n",
    "    \"prompt_col\": \"prompt\",\n",
    "    \"response_col\": \"response\",\n",
    "    \"label_col\": \"is_safe\",\n",
    "}\n",
    "\n",
    "dataset = PromptResponseDataset(dataframe=dataframe, config=data_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c952984",
   "metadata": {},
   "source": [
    "### Guardian model configuration\n",
    "Policy Distillation uses [Granite Guardian](https://www.ibm.com/granite/docs/models/guardian) as a LLM-as-a-Judge, and other Large Language Models (LLMs) to query at various stages in the pipeline. Therefore, it requires access to inference or call the models.\n",
    "\n",
    "In the cell below, an example of configuration for the guardian model has been provided. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93ef0055-4e03-4ed5-9806-b3fc3609cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardian model configuration\n",
    "guardian_config = {\n",
    "    \"task\": f\"Harm detection\",\n",
    "    \"criterion\": f\"Harm\",\n",
    "    \"criterion_definition\": f\"The message has harmful by common-sense.\",\n",
    "    \"labels\": [0, 1],\n",
    "    \"label_names\": [\"harmless\", \"harmful\"],\n",
    "    \"output_labels\": [\"no\", \"yes\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcadf6bf",
   "metadata": {},
   "source": [
    "**Available Inference Engines**: WML, Ollama, vLLM, RITS. Please follow the [Inference APIs](https://github.com/IBM/risk-atlas-nexus?tab=readme-ov-file#install-for-inference-apis) guide before going ahead.\n",
    "\n",
    "_Note:_ RITS is intended solely for internal IBM use and requires TUNNELALL VPN for access.\n",
    "\n",
    "Uncomment the section that is relevant for your use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afa02924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-02 11:16:35:450] - INFO - AIAtlasNexus - Created RITS inference engine.\n",
      "[2025-12-02 11:16:35:869] - INFO - AIAtlasNexus - Created RITS inference engine.\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "\n",
    "# WML\n",
    "\n",
    "# guardian_judge = WMLInferenceEngine(\n",
    "#     model_name_or_path=\"ibm/granite-guardian-3-8b\",\n",
    "#     credentials={\n",
    "#         \"api_key\": os.environ[\"WML_API_KEY\"],\n",
    "#         \"api_url\": os.environ[\"WML_API_URL\"],\n",
    "#         \"project_id\": os.environ[\"WML_PROJECT_ID\"],\n",
    "#     },\n",
    "#     parameters=WMLInferenceEngineParams(logprobs=True, top_logprobs=10, temperature=0),\n",
    "# )\n",
    "\n",
    "# llm_component = WMLInferenceEngine(\n",
    "#     model_name_or_path=\"meta-llama/llama-3-3-70b-instruct\", \n",
    "#     credentials={\n",
    "#         \"api_key\": os.environ[\"WML_API_KEY\"],\n",
    "#         \"api_url\": os.environ[\"WML_API_URL\"],\n",
    "#         \"project_id\": os.environ[\"WML_PROJECT_ID\"],\n",
    "#     },\n",
    "# )\n",
    "\n",
    "#############\n",
    "\n",
    "# VLLM\n",
    "\n",
    "# To run vLLM on an OpenAI-Compatible vLLM Server, execute the command:\n",
    "# vllm serve ibm-granite/granite-guardian-3.3-8b --max_model_len 2048 --host localhost --port 8000 --api-key <YOUR KEY>\n",
    "# vllm serve meta-llama/llama-3-3-70b-instruct --max_model_len 2048 --host localhost --port 8000 --api-key <YOUR KEY>\n",
    "\n",
    "# guardian_judge = VLLMInferenceEngine(\n",
    "#     model_name_or_path=\"ibm-granite/granite-guardian-3.3-8b\",\n",
    "#     credentials=InferenceEngineCredentials(\n",
    "#         api_url=os.environ[\"VLLM_API_URL\"], api_key=os.environ[\"VLLM_API_KEY\"]\n",
    "#     ),\n",
    "#     parameters=VLLMInferenceEngineParams(logprobs=True, temperature=0),\n",
    "# )\n",
    "# llm_component = VLLMInferenceEngine(\n",
    "#     model_name_or_path=\"meta-llama/Llama-3.3-70B-Instruct\", # gated model\n",
    "#     credentials=InferenceEngineCredentials(\n",
    "#     api_url=os.environ[\"VLLM_API_URL_LLM\"], api_key=os.environ[\"VLLM_API_KEY_LLM\"]\n",
    "# ),\n",
    "# )\n",
    "\n",
    "#############\n",
    "\n",
    "#RITS (IBM Internal Only, VPN required)\n",
    "\n",
    "guardian_judge = RITSInferenceEngine(\n",
    "    model_name_or_path=\"ibm-granite/granite-guardian-3.3-8b\",\n",
    "    credentials={\n",
    "        \"api_key\": os.environ[\"RITS_API_KEY\"],\n",
    "        \"api_url\": os.environ[\"RITS_API_URL\"],\n",
    "    },\n",
    "    parameters=RITSInferenceEngineParams(\n",
    "        logprobs=True, top_logprobs=10, temperature=0.0\n",
    "    ),\n",
    ")\n",
    "\n",
    "llm_component = RITSInferenceEngine(\n",
    "    model_name_or_path=\"meta-llama/llama-3-3-70b-instruct\",\n",
    "    credentials={\n",
    "        \"api_key\": os.environ[\"RITS_API_KEY\"],\n",
    "        \"api_url\": os.environ[\"RITS_API_URL\"],\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d419d3d-922e-4e10-aefe-3e8617e65e01",
   "metadata": {},
   "source": [
    "### Create and run the explanation generation pipeline\n",
    "\n",
    "The pipeline streamlines local and global explanation generation process. The Extractor executes the CLoVE algorithm and generates a set of local explanations, and Clusterer executes GloVE algorithm and merges the local explanations into a global one.\n",
    "\n",
    "Pass `lime=False` to pipeline creation step if no local word-based verification is done. Similarly, use `fr=False` if FactReasoner is not used to verify global explanations.\n",
    "\n",
    "The resulting local and global explanations are saved in the path folder passed to the pipeline.run() call.\n",
    "\n",
    "The execution logs can be found in the logs folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aee994e0-9e5d-4903-81af-22c78c240fb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_policy_rules(guardian_config, dataset: AbstractDataset,guardian_judge: InferenceEngine,llm_component: InferenceEngine,local_expl: Literal[\"LIME\", \"SHAP\"] = \"LIME\",results_path: Path = Path(\"results\")):\n",
    "    \"\"\"Generate the policy rules.\n",
    "\n",
    "    Args:\n",
    "        guardian_config (Dict): guardian config,\n",
    "        dataset (AbstractDataset): Dataset to be used for running the pipeline,\n",
    "        guardian_judge (InferenceEngine): An LLM inference engine instance of the Granite Guardian,\n",
    "        llm_component (InferenceEngine): An LLM inference engine instance for all steps of the policy distillation pipeline\n",
    "        local_expl (Literal[&quot;LIME&quot;, &quot;SHAP&quot;], optional): local explanation model -- only LIME and SHAP are supported. Defaults to \"LIME\".\n",
    "        results_path (Path, optional): Output directory path. Defaults to Path(\"results\").\n",
    "\n",
    "    Returns:\n",
    "        List: A list of policy rules\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an instance of the guardian model\n",
    "    guardian = Guardian(\n",
    "        inference_engine=guardian_judge,\n",
    "        config=guardian_config,\n",
    "    )\n",
    "\n",
    "    # local explanation model -- only LIME and SHAP are supported\n",
    "    if local_expl == \"LIME\":\n",
    "        local_explainer = LIME(\n",
    "            dataset.dataset_name, guardian_config[\"label_names\"], n_samples=100\n",
    "        )\n",
    "    elif local_expl == \"SHAP\":\n",
    "        local_explainer = SHAP(\n",
    "            dataset.dataset_name, guardian_config[\"label_names\"], n_samples=100\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Only LIME and SHAP are supported\")\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(\n",
    "        extractor=Extractor(\n",
    "            guardian,\n",
    "            llm_component,\n",
    "            guardian_config[\"criterion\"],\n",
    "            guardian_config[\"criterion_definition\"],\n",
    "            local_explainer,\n",
    "        ),\n",
    "        clusterer=Clusterer(\n",
    "            llm_component,\n",
    "            guardian_config[\"criterion_definition\"],\n",
    "            guardian_config[\"label_names\"],\n",
    "            n_iter=10,\n",
    "        ),\n",
    "        lime=True,\n",
    "        fr=True,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Run pipeline\n",
    "    expl = pipeline.run(dataset, results_path=results_path)\n",
    "    return expl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "103d2fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-02 11:16:35] INFO pipeline.py:50: Built pipeline.\n",
      "[2025-12-02 11:16:35] INFO pipeline.py:51: Using LIME = True\n",
      "[2025-12-02 11:16:35] INFO pipeline.py:52: Using FactReasoner = True\n",
      "[2025-12-02 11:16:35] INFO pipeline.py:69: Results directory for BeaverTails: [results/BeaverTails/local/local_expl.csv, results/BeaverTails/global/global_expl.pkl]\n",
      "[2025-12-02 11:16:35] INFO concept_extractor.py:71: Generating local explanations...\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\n",
      "Inferring with RITS: 100%|██████████| 100/100 [00:05<00:00, 19.10it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.96it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  2.14it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "Inferring with RITS: 100%|██████████| 100/100 [00:04<00:00, 21.58it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:01<00:00,  1.61s/it]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  2.05it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  2.09it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  2.04it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.82it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "Inferring with RITS: 100%|██████████| 100/100 [00:04<00:00, 20.02it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.86it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.82it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\n",
      "3it [00:38, 12.98s/it]\n",
      "[2025-12-02 11:17:14] INFO concept_extractor.py:119: Explained 3 instances. Results saved in results/BeaverTails/local/local_expl.csv\n",
      "/Users/ingevejs/Documents/workspace/ingelise/risk-atlas-nexus-demos/risk-policy-distillation/.venv-risk-policy-distillation/lib/python3.12/site-packages/risk_policy_distillation/pipeline/pipeline.py:143: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cds[\"Zipped\"] = cds.apply(\n",
      "[2025-12-02 11:17:14] INFO bipartite_graph.py:106: Loaded the following graph:\n",
      "\tLabels = dict_keys([0, 1]) Sizes = [2, 0] Number of edges = 0\n",
      "/Users/ingevejs/Documents/workspace/ingelise/risk-atlas-nexus-demos/risk-policy-distillation/.venv-risk-policy-distillation/lib/python3.12/site-packages/risk_policy_distillation/pipeline/pipeline.py:143: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cds[\"Zipped\"] = cds.apply(\n",
      "[2025-12-02 11:17:14] INFO bipartite_graph.py:106: Loaded the following graph:\n",
      "\tLabels = dict_keys([0, 1]) Sizes = [8, 9] Number of edges = 6\n",
      "[2025-12-02 11:17:14] INFO clusterer.py:187: Clustering...\n",
      "[2025-12-02 11:17:14] INFO clusterer.py:42: Clustering 8 instances\n",
      "[2025-12-02 11:17:14] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:17:14] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.48s/it]\n",
      "[2025-12-02 11:17:20] INFO clusterer.py:69: Cleaned up 2 clusters with 6 total concepts\n",
      "[2025-12-02 11:17:20] INFO clusterer.py:75: ['no encouragement', 'no encouragement', 'no encouragement']\n",
      "[2025-12-02 11:17:20] INFO bipartite_graph.py:119: \n",
      "\t\t\tMerging 3 nodes on 0 side.\n",
      "[2025-12-02 11:17:20] INFO bipartite_graph.py:134: \t\tAdded a node: id = label = 8, probability = no encouragement, num of subnodes = 1.0\n",
      "[2025-12-02 11:17:20] INFO clusterer.py:75: ['existing phrases listed', 'existing phrases listed', 'existing phrases listed']\n",
      "[2025-12-02 11:17:20] INFO bipartite_graph.py:119: \n",
      "\t\t\tMerging 3 nodes on 0 side.\n",
      "[2025-12-02 11:17:20] INFO bipartite_graph.py:134: \t\tAdded a node: id = label = 9, probability = existing phrases listed, num of subnodes = 1.0\n",
      "[2025-12-02 11:17:20] INFO clusterer.py:42: Clustering 9 instances\n",
      "[2025-12-02 11:17:20] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:17:20] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n",
      "[2025-12-02 11:17:23] INFO clusterer.py:69: Cleaned up 3 clusters with 9 total concepts\n",
      "[2025-12-02 11:17:23] INFO clusterer.py:75: ['hurtful language', 'hurtful language', 'hurtful language']\n",
      "[2025-12-02 11:17:23] INFO bipartite_graph.py:119: \n",
      "\t\t\tMerging 3 nodes on 1 side.\n",
      "[2025-12-02 11:17:23] INFO bipartite_graph.py:134: \t\tAdded a node: id = label = 9, probability = hurtful language, num of subnodes = 1.0\n",
      "[2025-12-02 11:17:23] INFO clusterer.py:75: ['disrespectful tone', 'disrespectful tone', 'disrespectful tone']\n",
      "[2025-12-02 11:17:23] INFO bipartite_graph.py:119: \n",
      "\t\t\tMerging 3 nodes on 1 side.\n",
      "[2025-12-02 11:17:23] INFO bipartite_graph.py:134: \t\tAdded a node: id = label = 10, probability = disrespectful tone, num of subnodes = 1.0\n",
      "[2025-12-02 11:17:23] INFO clusterer.py:75: ['derogatory terms used', 'derogatory terms used', 'derogatory terms used']\n",
      "[2025-12-02 11:17:23] INFO bipartite_graph.py:119: \n",
      "\t\t\tMerging 3 nodes on 1 side.\n",
      "[2025-12-02 11:17:23] INFO bipartite_graph.py:134: \t\tAdded a node: id = label = 11, probability = derogatory terms used, num of subnodes = 1.0\n",
      "[2025-12-02 11:17:23] INFO clusterer.py:42: Clustering 4 instances\n",
      "[2025-12-02 11:17:23] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:17:23] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.90it/s]\n",
      "[2025-12-02 11:17:25] INFO clusterer.py:42: Clustering 4 instances\n",
      "[2025-12-02 11:17:25] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:17:25] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.15it/s]\n",
      "[2025-12-02 11:17:28] INFO clusterer.py:69: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-02 11:17:28] INFO clusterer.py:42: Clustering 3 instances\n",
      "[2025-12-02 11:17:28] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:17:28] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.04it/s]\n",
      "[2025-12-02 11:17:31] INFO clusterer.py:69: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-02 11:17:31] INFO clusterer.py:42: Clustering 3 instances\n",
      "[2025-12-02 11:17:31] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:17:31] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.16it/s]\n",
      "[2025-12-02 11:17:33] INFO clusterer.py:42: Clustering 4 instances\n",
      "[2025-12-02 11:17:33] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:17:33] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.96it/s]\n",
      "[2025-12-02 11:17:36] INFO clusterer.py:69: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-02 11:17:36] INFO clusterer.py:42: Clustering 3 instances\n",
      "[2025-12-02 11:17:36] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:17:36] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.26it/s]\n",
      "[2025-12-02 11:17:38] INFO clusterer.py:69: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-02 11:17:38] INFO clusterer.py:42: Clustering 4 instances\n",
      "[2025-12-02 11:17:38] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:17:38] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.76it/s]\n",
      "[2025-12-02 11:17:41] INFO clusterer.py:42: Clustering 4 instances\n",
      "[2025-12-02 11:17:41] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:17:41] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.17it/s]\n",
      "[2025-12-02 11:17:44] INFO clusterer.py:69: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-02 11:17:44] INFO clusterer.py:42: Clustering 3 instances\n",
      "[2025-12-02 11:17:44] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:17:44] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.65it/s]\n",
      "[2025-12-02 11:17:46] INFO clusterer.py:69: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-02 11:17:46] INFO clusterer.py:42: Clustering 3 instances\n",
      "[2025-12-02 11:17:46] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:17:46] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.18it/s]\n",
      "[2025-12-02 11:17:49] INFO clusterer.py:42: Clustering 4 instances\n",
      "[2025-12-02 11:17:49] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:17:49] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.64it/s]\n",
      "[2025-12-02 11:17:51] INFO clusterer.py:69: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-02 11:17:51] INFO clusterer.py:42: Clustering 3 instances\n",
      "[2025-12-02 11:17:51] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:17:51] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.27it/s]\n",
      "[2025-12-02 11:17:54] INFO clusterer.py:69: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-02 11:17:54] INFO clusterer.py:42: Clustering 4 instances\n",
      "[2025-12-02 11:17:54] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:17:54] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.50it/s]\n",
      "[2025-12-02 11:17:56] INFO clusterer.py:42: Clustering 4 instances\n",
      "[2025-12-02 11:17:56] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:17:56] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.47it/s]\n",
      "[2025-12-02 11:17:59] INFO clusterer.py:69: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-02 11:17:59] INFO clusterer.py:42: Clustering 3 instances\n",
      "[2025-12-02 11:17:59] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:17:59] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.07it/s]\n",
      "[2025-12-02 11:18:01] INFO clusterer.py:69: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-02 11:18:01] INFO clusterer.py:42: Clustering 3 instances\n",
      "[2025-12-02 11:18:01] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:18:01] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.23it/s]\n",
      "[2025-12-02 11:18:04] INFO clusterer.py:42: Clustering 4 instances\n",
      "[2025-12-02 11:18:04] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:18:04] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.40it/s]\n",
      "[2025-12-02 11:18:06] INFO clusterer.py:69: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-02 11:18:06] INFO clusterer.py:42: Clustering 3 instances\n",
      "[2025-12-02 11:18:06] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:18:06] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.73it/s]\n",
      "[2025-12-02 11:18:09] INFO clusterer.py:69: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-02 11:18:09] INFO clusterer.py:42: Clustering 4 instances\n",
      "[2025-12-02 11:18:09] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:18:09] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.87it/s]\n",
      "[2025-12-02 11:18:11] INFO clusterer.py:42: Clustering 4 instances\n",
      "[2025-12-02 11:18:11] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:18:11] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.21it/s]\n",
      "[2025-12-02 11:18:14] INFO clusterer.py:69: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-02 11:18:14] INFO clusterer.py:42: Clustering 3 instances\n",
      "[2025-12-02 11:18:14] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:18:14] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.66it/s]\n",
      "[2025-12-02 11:18:17] INFO clusterer.py:69: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-02 11:18:17] INFO clusterer.py:42: Clustering 3 instances\n",
      "[2025-12-02 11:18:17] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:18:17] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.38it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "\u001b[92m11:18:20 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:20] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:20 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:20] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m11:18:21 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:21] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m11:18:21 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:21] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 40920.04prompts/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
      "\u001b[92m11:18:21 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:21] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:21 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:21] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m11:18:21 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:21] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m11:18:21 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:21] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 49932.19prompts/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  2.05it/s]\n",
      "\u001b[92m11:18:22 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:22] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:22 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:22] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m11:18:22 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:22] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m11:18:22 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:22] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 77672.30prompts/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n",
      "\u001b[92m11:18:22 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:22] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:22 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:22] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:23 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:23] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m11:18:23 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:23] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 42581.77prompts/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n",
      "\u001b[92m11:18:23 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:23] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:23 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:23] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m11:18:23 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:23] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m11:18:23 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:23] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 66576.25prompts/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\n",
      "\u001b[92m11:18:24 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:24] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:24 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:24] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m11:18:24 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:24] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m11:18:24 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:24] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 74235.47prompts/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  2.02it/s]\n",
      "\u001b[92m11:18:25 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:25] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:25 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:25] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:25 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:25] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m11:18:25 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:25] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 64527.75prompts/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\n",
      "\u001b[92m11:18:25 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:25] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:25 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:25] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:26 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:26] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m11:18:26 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:26] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 55924.05prompts/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\u001b[92m11:18:26 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:26] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:26 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:26] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m11:18:27 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:27] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m11:18:27 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:27] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 48770.98prompts/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s]\n",
      "\u001b[92m11:18:27 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:27] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:27 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:27] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m11:18:27 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:27] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m11:18:27 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:27] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 53773.13prompts/s]\n",
      "[2025-12-02 11:18:27] INFO clusterer.py:42: Clustering 4 instances\n",
      "[2025-12-02 11:18:27] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:18:27] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.07it/s]\n",
      "[2025-12-02 11:18:30] INFO clusterer.py:69: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-02 11:18:30] INFO clusterer.py:42: Clustering 3 instances\n",
      "[2025-12-02 11:18:30] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:18:30] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.89it/s]\n",
      "[2025-12-02 11:18:33] INFO clusterer.py:69: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-02 11:18:33] INFO clusterer.py:42: Clustering 4 instances\n",
      "[2025-12-02 11:18:33] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:18:33] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.30it/s]\n",
      "[2025-12-02 11:18:35] INFO clusterer.py:42: Clustering 4 instances\n",
      "[2025-12-02 11:18:35] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:18:35] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.67it/s]\n",
      "[2025-12-02 11:18:38] INFO clusterer.py:69: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-02 11:18:38] INFO clusterer.py:42: Clustering 3 instances\n",
      "[2025-12-02 11:18:38] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:18:38] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.85it/s]\n",
      "[2025-12-02 11:18:40] INFO clusterer.py:69: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-02 11:18:40] INFO clusterer.py:42: Clustering 3 instances\n",
      "[2025-12-02 11:18:40] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-02 11:18:40] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 48.85it/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\n",
      "\u001b[92m11:18:44 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:44] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:44 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:44] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m11:18:44 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:44] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m11:18:44 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:44] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 57065.36prompts/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\u001b[92m11:18:45 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:45] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:45 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:45] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m11:18:45 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:45] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m11:18:45 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:45] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 66576.25prompts/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      "\u001b[92m11:18:45 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:45] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:45 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:45] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:45 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:45] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m11:18:45 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:45] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 51150.05prompts/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\n",
      "\u001b[92m11:18:46 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:46] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:46 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:46] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:46 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:46] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m11:18:46 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:46] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 53773.13prompts/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "\u001b[92m11:18:47 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:47] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:47 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:47] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:47 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:47] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m11:18:47 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:47] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 54120.05prompts/s]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      "\u001b[92m11:18:47 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:47] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:47 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "[2025-12-02 11:18:47] INFO utils.py:3419: \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m11:18:47 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:47] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m11:18:47 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
      "[2025-12-02 11:18:47] INFO utils.py:1308: Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 43464.29prompts/s]\n",
      "[2025-12-02 11:18:47] INFO bipartite_graph.py:119: \n",
      "\t\t\tMerging 2 nodes on 1 side.\n",
      "[2025-12-02 11:18:47] INFO bipartite_graph.py:134: \t\tAdded a node: id = label = 12, probability = uses disrespectful language, num of subnodes = 1.0\n",
      "[2025-12-02 11:18:47] INFO global_expl.py:34: Loaded 2 rules\n",
      "[2025-12-02 11:18:47] INFO global_expl.py:134: Stored global explanation to results/BeaverTails/global/global_expl.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rules': [{'prediction': 1,\n",
       "   'if_clause': 'uses disrespectful language',\n",
       "   'despite_clauses': 'none'},\n",
       "  {'prediction': 1,\n",
       "   'if_clause': 'derogatory terms used',\n",
       "   'despite_clauses': 'none'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the rules\n",
    "expl = generate_policy_rules(\n",
    "    guardian_config, dataset, guardian_judge, llm_component, local_expl=\"LIME\"\n",
    ")\n",
    "\n",
    "#print the output\n",
    "expl.print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-risk-policy-distillation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
