{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92b30a6-4a3c-4215-a0b8-3f61480ccba3",
   "metadata": {},
   "source": [
    "# Generating global explanations of LLM-as-a-Judge using GloVE algorithm\n",
    "\n",
    "This notebook shows how you might run the full pipeline and generate a global summary given a dataset and an LLM-as-a-Judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11e8712c-f06a-4bad-970b-fdcf85c3aa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from risk_policy_distillation.datasets.prompt_response_dataset import (\n",
    "    PromptResponseDataset,\n",
    ")\n",
    "from risk_policy_distillation.datasets.abs_dataset import AbstractDataset\n",
    "from risk_policy_distillation.models.explainers.local_explainers.lime import LIME\n",
    "from risk_policy_distillation.models.explainers.local_explainers.shap_vals import SHAP\n",
    "from risk_policy_distillation.models.guardians.guardian import Guardian\n",
    "from risk_policy_distillation.pipeline.clusterer import Clusterer\n",
    "from risk_policy_distillation.pipeline.concept_extractor import Extractor\n",
    "from risk_policy_distillation.pipeline.pipeline import Pipeline\n",
    "\n",
    "# use risk atlas nexus for the inference tasks\n",
    "from risk_atlas_nexus.blocks.inference import (\n",
    "    InferenceEngine,\n",
    "    RITSInferenceEngine,\n",
    "    WMLInferenceEngine,\n",
    "    OllamaInferenceEngine,\n",
    "    VLLMInferenceEngine,\n",
    ")\n",
    "from risk_atlas_nexus.blocks.inference.params import (\n",
    "    InferenceEngineCredentials,\n",
    "    RITSInferenceEngineParams,\n",
    "    WMLInferenceEngineParams,\n",
    "    OllamaInferenceEngineParams,\n",
    "    VLLMInferenceEngineParams,\n",
    ")\n",
    "\n",
    "from risk_atlas_nexus.library import RiskAtlasNexus\n",
    "\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f2b11f-d205-4f81-93bb-9ab3dd785db8",
   "metadata": {},
   "source": [
    "## Task\n",
    "Explain the LLM-as-a-Judge output. \n",
    "\n",
    "## Create a dataset\n",
    "To explain the LLM-as-a-Judge, a dataset must be provided. [AbstractDataset](../../src/risk_policy_distillation/datasets/abs_dataset.py) class provides a wrapper for a dataframe you want to explain. You can use [PromptDataset](../../src/risk_policy_distillation/datasets/prompt_dataset.py) or [PromptResponseDataset](../../src/risk_policy_distillation/datasets/prompt_response_dataset.py) depending on whether your dataframe consists of only prompts or prompt-response pairs. You can also create a custom dataset by inheriting the Dataset class.\n",
    "\n",
    "### Setup dataset configuration\n",
    "In the cell below, an example of configuration with information on column name mapping is shown. \n",
    "\n",
    "A small sample from the dataset [PKU-Alignment/BeaverTails](https://github.com/PKU-Alignment/beavertails) is chosen to illustrate the example.  BeaverTails was developed to support research on safety alignment in large language models (LLMs), and consists of 300k+ human-labeled question-answering (QA) pairs, each associated with specific harm categories.  This sample and configuration are used to create a PromptResponseDataset.\n",
    "\n",
    "Additional parameters: \n",
    "\n",
    "_flip_labels_ indicates whether labels of the dataframe should be flipped in preprocessing step (e.g. for BeaverTails where labels indicate that the content is safe rather than harmful); \n",
    "\n",
    "_split_ indicates whether a train-val-test split needs to be performed during preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7bc4d137-23e8-47da-afbd-88947bc07643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "ds = load_dataset(\"PKU-Alignment/BeaverTails\")\n",
    "dataframe = ds.data[\"330k_train\"].table.to_pandas()\n",
    "\n",
    "# sample for testing purposes\n",
    "dataframe = dataframe.sample(10)\n",
    "\n",
    "# provide some information in the data config\n",
    "data_config = {\n",
    "    \"dataset_name\": \"BeaverTails\",\n",
    "    \"index_col\": \"\",  # if there is no index column it will be automatically populated\n",
    "    \"prompt_col\": \"prompt\",\n",
    "    \"response_col\": \"response\",\n",
    "    \"label_col\": \"is_safe\",\n",
    "}\n",
    "\n",
    "dataset = PromptResponseDataset(dataframe=dataframe, config=data_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c952984",
   "metadata": {},
   "source": [
    "### Guardian model configuration\n",
    "Policy Distillation uses [Granite Guardian](https://www.ibm.com/granite/docs/models/guardian) as a LLM-as-a-Judge, and other Large Language Models (LLMs) to query at various stages in the pipeline. Therefore, it requires access to inference or call the models.\n",
    "\n",
    "In the cell below, an example of configuration for the guardian model has been provided. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93ef0055-4e03-4ed5-9806-b3fc3609cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardian model configuration\n",
    "guardian_config = {\n",
    "    \"task\": f\"Harm detection\",\n",
    "    \"criterion\": f\"Harm\",\n",
    "    \"criterion_definition\": f\"The message has harmful by common-sense.\",\n",
    "    \"labels\": [0, 1],\n",
    "    \"label_names\": [\"harmless\", \"harmful\"],\n",
    "    \"output_labels\": [\"no\", \"yes\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcadf6bf",
   "metadata": {},
   "source": [
    "**Available Inference Engines**: WML, Ollama, vLLM, RITS. Please follow the [Inference APIs](https://github.com/IBM/risk-atlas-nexus?tab=readme-ov-file#install-for-inference-apis) guide before going ahead.\n",
    "\n",
    "_Note:_ RITS is intended solely for internal IBM use and requires TUNNELALL VPN for access.\n",
    "\n",
    "Uncomment the section that is relevant for your use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "afa02924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-25 20:27:25:422] - INFO - RiskAtlasNexus - Created RITS inference engine.\n",
      "[2025-11-25 20:27:25:938] - INFO - RiskAtlasNexus - Created RITS inference engine.\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "\n",
    "# WML\n",
    "\n",
    "# guardian_judge = WMLInferenceEngine(\n",
    "#     model_name_or_path=\"ibm/granite-guardian-3-8b\",\n",
    "#     credentials={\n",
    "#         \"api_key\": os.environ[\"WML_API_KEY\"],\n",
    "#         \"api_url\": os.environ[\"WML_API_URL\"],\n",
    "#         \"project_id\": os.environ[\"WML_PROJECT_ID\"],\n",
    "#     },\n",
    "#     parameters=WMLInferenceEngineParams(logprobs=True, top_logprobs=10, temperature=0),\n",
    "# )\n",
    "\n",
    "# llm_component = WMLInferenceEngine(\n",
    "#     model_name_or_path=\"meta-llama/llama-3-3-70b-instruct\", \n",
    "#     credentials={\n",
    "#         \"api_key\": os.environ[\"WML_API_KEY\"],\n",
    "#         \"api_url\": os.environ[\"WML_API_URL\"],\n",
    "#         \"project_id\": os.environ[\"WML_PROJECT_ID\"],\n",
    "#     },\n",
    "# )\n",
    "\n",
    "#############\n",
    "\n",
    "# VLLM\n",
    "\n",
    "# To run vLLM on an OpenAI-Compatible vLLM Server, execute the command:\n",
    "# vllm serve ibm-granite/granite-guardian-3.3-8b --max_model_len 2048 --host localhost --port 8000 --api-key <YOUR KEY>\n",
    "# vllm serve meta-llama/llama-3-3-70b-instruct --max_model_len 2048 --host localhost --port 8000 --api-key <YOUR KEY>\n",
    "\n",
    "# guardian_judge = VLLMInferenceEngine(\n",
    "#     model_name_or_path=\"ibm-granite/granite-guardian-3.3-8b\",\n",
    "#     credentials=InferenceEngineCredentials(\n",
    "#         api_url=os.environ[\"VLLM_API_URL\"], api_key=os.environ[\"VLLM_API_KEY\"]\n",
    "#     ),\n",
    "#     parameters=VLLMInferenceEngineParams(logprobs=True, temperature=0),\n",
    "# )\n",
    "# llm_component = VLLMInferenceEngine(\n",
    "#     model_name_or_path=\"meta-llama/Llama-3.3-70B-Instruct\", # gated model\n",
    "#     credentials=InferenceEngineCredentials(\n",
    "#     api_url=os.environ[\"VLLM_API_URL_LLM\"], api_key=os.environ[\"VLLM_API_KEY_LLM\"]\n",
    "# ),\n",
    "# )\n",
    "\n",
    "#############\n",
    "\n",
    "#RITS (IBM Internal Only, VPN required)\n",
    "\n",
    "guardian_judge = RITSInferenceEngine(\n",
    "    model_name_or_path=\"ibm-granite/granite-guardian-3.3-8b\",\n",
    "    credentials={\n",
    "        \"api_key\": os.environ[\"RITS_API_KEY\"],\n",
    "        \"api_url\": os.environ[\"RITS_API_URL\"],\n",
    "    },\n",
    "    parameters=RITSInferenceEngineParams(\n",
    "        logprobs=True, top_logprobs=10, temperature=0.0\n",
    "    ),\n",
    ")\n",
    "\n",
    "llm_component = RITSInferenceEngine(\n",
    "    model_name_or_path=\"meta-llama/llama-3-3-70b-instruct\",\n",
    "    credentials={\n",
    "        \"api_key\": os.environ[\"RITS_API_KEY\"],\n",
    "        \"api_url\": os.environ[\"RITS_API_URL\"],\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d419d3d-922e-4e10-aefe-3e8617e65e01",
   "metadata": {},
   "source": [
    "### Create and run the explanation generation pipeline\n",
    "\n",
    "The pipeline streamlines local and global explanation generation process. The Extractor executes the CLoVE algorithm and generates a set of local explanations, and Clusterer executes GloVE algorithm and merges the local explanations into a global one.\n",
    "\n",
    "Pass `lime=False` to pipeline creation step if no local word-based verification is done. Similarly, use `fr=False` if FactReasoner is not used to verify global explanations.\n",
    "\n",
    "The resulting local and global explanations are saved in the path folder passed to the pipeline.run() call.\n",
    "\n",
    "The execution logs can be found in the logs folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aee994e0-9e5d-4903-81af-22c78c240fb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_policy_rules(guardian_config, dataset: AbstractDataset,guardian_judge: InferenceEngine,llm_component: InferenceEngine,local_expl: Literal[\"LIME\", \"SHAP\"] = \"LIME\",results_path: Path = Path(\"results\")):\n",
    "    \"\"\"Generate the policy rules.\n",
    "\n",
    "    Args:\n",
    "        guardian_config (Dict): guardian connfig,\n",
    "        dataset (AbstractDataset): Dataset to be used for running the pipeline,\n",
    "        guardian_judge (InferenceEngine): An LLM inference engine instance of the Granite Guardian,\n",
    "        llm_component (InferenceEngine): An LLM inference engine instance for all steps of the policy distillation pipeline\n",
    "        local_expl (Literal[&quot;LIME&quot;, &quot;SHAP&quot;], optional): local explanation model -- only LIME and SHAP are supported. Defaults to \"LIME\".\n",
    "        results_path (Path, optional): Output directory path. Defaults to Path(\"results\").\n",
    "\n",
    "    Returns:\n",
    "        List: A list of policy rules\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an instance of the guardian model\n",
    "    guardian = Guardian(\n",
    "        inference_engine=guardian_judge,\n",
    "        config=guardian_config,\n",
    "    )\n",
    "\n",
    "    # local explanation model -- only LIME and SHAP are supported\n",
    "    if local_expl == \"LIME\":\n",
    "        local_explainer = LIME(\n",
    "            dataset.dataset_name, guardian_config[\"label_names\"], n_samples=100\n",
    "        )\n",
    "    elif local_expl == \"SHAP\":\n",
    "        local_explainer = SHAP(\n",
    "            dataset.dataset_name, guardian_config[\"label_names\"], n_samples=100\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Only LIME and SHAP are supported\")\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(\n",
    "        extractor=Extractor(\n",
    "            guardian,\n",
    "            llm_component,\n",
    "            guardian_config[\"criterion\"],\n",
    "            guardian_config[\"criterion_definition\"],\n",
    "            local_explainer,\n",
    "        ),\n",
    "        clusterer=Clusterer(\n",
    "            llm_component,\n",
    "            guardian_config[\"criterion_definition\"],\n",
    "            guardian_config[\"label_names\"],\n",
    "            n_iter=10,\n",
    "        ),\n",
    "        lime=True,\n",
    "        fr=True,\n",
    "    )\n",
    "\n",
    "    # Run pipeline\n",
    "    expl = pipeline.run(dataset, results_path=results_path)\n",
    "    return expl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "103d2fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rules': [{'prediction': 0, 'if_clause': 'neutral explanation', 'despite_clauses': 'none'}]}\n"
     ]
    }
   ],
   "source": [
    "# generate the rules\n",
    "expl = generate_policy_rules(\n",
    "    guardian_config, dataset, guardian_judge, llm_component, local_expl=\"LIME\"\n",
    ")\n",
    "\n",
    "print(expl.print())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-risk-policy-distillation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
