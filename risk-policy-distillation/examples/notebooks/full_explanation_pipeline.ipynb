{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92b30a6-4a3c-4215-a0b8-3f61480ccba3",
   "metadata": {},
   "source": [
    "# Generating global explanations of LLM-as-a-Judge using GloVE algorithm\n",
    "\n",
    "This notebook shows how you might run the full pipeline and generate a global summary given a dataset and an LLM-as-a-Judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b04adfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e8712c-f06a-4bad-970b-fdcf85c3aa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ingevejs/Documents/workspace/ingelise/risk-atlas-nexus-demos/risk-policy-distillation/.venv-risk-policy-distillation/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-04 12:54:18 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-04 12:54:19] INFO loader.py:156: Loading faiss.\n",
      "[2025-12-04 12:54:19] INFO loader.py:158: Successfully loaded faiss.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from risk_policy_distillation.datasets.prompt_response_dataset import (\n",
    "    PromptResponseDataset,\n",
    ")\n",
    "from risk_policy_distillation.datasets.abs_dataset import AbstractDataset\n",
    "from risk_policy_distillation.models.explainers.local_explainers.lime import LIME\n",
    "from risk_policy_distillation.models.explainers.local_explainers.shap_vals import SHAP\n",
    "from risk_policy_distillation.models.guardians.guardian import Guardian\n",
    "from risk_policy_distillation.pipeline.clusterer import Clusterer\n",
    "from risk_policy_distillation.pipeline.concept_extractor import Extractor\n",
    "from risk_policy_distillation.pipeline.pipeline import Pipeline\n",
    "\n",
    "# use AI atlas nexus for the inference tasks\n",
    "from ai_atlas_nexus.blocks.inference import (\n",
    "    InferenceEngine,\n",
    "    RITSInferenceEngine,\n",
    "    OllamaInferenceEngine,\n",
    "    VLLMInferenceEngine,\n",
    ")\n",
    "from ai_atlas_nexus.blocks.inference.params import (\n",
    "    InferenceEngineCredentials,\n",
    "    RITSInferenceEngineParams,\n",
    "    OllamaInferenceEngineParams,\n",
    "    VLLMInferenceEngineParams,\n",
    ")\n",
    "\n",
    "from ai_atlas_nexus.library import AIAtlasNexus\n",
    "\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f2b11f-d205-4f81-93bb-9ab3dd785db8",
   "metadata": {},
   "source": [
    "## Task\n",
    "Explain output from an LLM-as-a-Judge. \n",
    "\n",
    "## Create a dataset\n",
    "To explain the LLM-as-a-Judge, a dataset must be provided. [AbstractDataset](../../src/risk_policy_distillation/datasets/abs_dataset.py) class provides a wrapper for a dataframe you want to explain. You can use [PromptDataset](../../src/risk_policy_distillation/datasets/prompt_dataset.py) or [PromptResponseDataset](../../src/risk_policy_distillation/datasets/prompt_response_dataset.py) depending on whether your dataframe consists of only prompts or prompt-response pairs. You can also create a custom dataset by inheriting the Dataset class.\n",
    "\n",
    "### Setup dataset configuration\n",
    "In the cell below, an example of configuration with information on column name mapping is shown. \n",
    "\n",
    "A small sample from the dataset [PKU-Alignment/BeaverTails](https://github.com/PKU-Alignment/beavertails) is chosen to illustrate the example.  BeaverTails was developed to support research on safety alignment in large language models (LLMs), and consists of 300k+ human-labeled question-answering (QA) pairs, each associated with specific harm categories.  This sample and configuration are used to create a PromptResponseDataset.\n",
    "\n",
    "Additional parameters: \n",
    "\n",
    "_flip_labels_ indicates whether labels of the dataframe should be flipped in preprocessing step (e.g. for BeaverTails where labels indicate that the content is safe rather than harmful); \n",
    "\n",
    "_split_ indicates whether a train-val-test split needs to be performed during preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bc4d137-23e8-47da-afbd-88947bc07643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "ds = load_dataset(\"PKU-Alignment/BeaverTails\")\n",
    "dataframe = ds.data[\"330k_train\"].table.to_pandas()\n",
    "\n",
    "# sample for testing purposes\n",
    "dataframe = dataframe.sample(3)\n",
    "\n",
    "# provide some information in the data config\n",
    "data_config = {\n",
    "    \"dataset_name\": \"BeaverTails\",\n",
    "    \"index_col\": \"\",  # if there is no index column it will be automatically populated\n",
    "    \"prompt_col\": \"prompt\",\n",
    "    \"response_col\": \"response\",\n",
    "    \"label_col\": \"is_safe\",\n",
    "}\n",
    "\n",
    "dataset = PromptResponseDataset(dataframe=dataframe, config=data_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c952984",
   "metadata": {},
   "source": [
    "### Guardian model configuration\n",
    "Policy Distillation uses [Granite Guardian](https://www.ibm.com/granite/docs/models/guardian) as a LLM-as-a-Judge, and other Large Language Models (LLMs) to query at various stages in the pipeline. Therefore, it requires access to inference or call the models.\n",
    "\n",
    "In the cell below, an example of configuration for the guardian model has been provided. Note, the output labels are case sensitive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef0055-4e03-4ed5-9806-b3fc3609cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardian model configuration\n",
    "guardian_config = {\n",
    "    \"task\": f\"Harm detection\",\n",
    "    \"criterion\": f\"Harm\",\n",
    "    \"criterion_definition\": f\"The message has harmful by common-sense.\",\n",
    "    \"labels\": [0, 1],\n",
    "    \"label_names\": [\"harmless\", \"harmful\"],\n",
    "    \"output_labels\": [\"No\", \"Yes\"], \n",
    "    #\"output_labels\": [\"no\", \"yes\"],  # case sensitive, use these for RITS\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcadf6bf",
   "metadata": {},
   "source": [
    "**Available Inference Engines**: Ollama, vLLM, RITS. Please follow the [Inference APIs](https://github.com/IBM/risk-atlas-nexus?tab=readme-ov-file#install-for-inference-apis) guide before going ahead.\n",
    "\n",
    "_Note:_ RITS is intended solely for internal IBM use and requires TUNNELALL VPN for access.\n",
    "\n",
    "Uncomment the section that is relevant for your use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa02924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-04 12:54:22:998] - INFO - AIAtlasNexus - OLLAMA inference engine will execute requests on the server at http://localhost:11434.\n",
      "[2025-12-04 12:54:23:63] - INFO - AIAtlasNexus - Created OLLAMA inference engine.\n",
      "[2025-12-04 12:54:23:63] - INFO - AIAtlasNexus - OLLAMA inference engine will execute requests on the server at http://localhost:11434.\n",
      "[2025-12-04 12:54:23:87] - INFO - AIAtlasNexus - Created OLLAMA inference engine.\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "# VLLM\n",
    "\n",
    "# To run vLLM on an OpenAI-Compatible vLLM Server, ensure VLLM in installed, and then execute the command:\n",
    "# vllm serve ibm-granite/granite-guardian-3.3-8b --max_model_len 2048 --host localhost --port 8000 --api-key <YOUR KEY>\n",
    "# vllm serve meta-llama/llama-3-3-70b-instruct --max_model_len 2048 --host localhost --port 8000 --api-key <YOUR KEY>\n",
    "\n",
    "# guardian_judge = VLLMInferenceEngine(\n",
    "#     model_name_or_path=\"ibm-granite/granite-guardian-3.3-8b\",\n",
    "#     credentials=InferenceEngineCredentials(\n",
    "#         api_url=os.environ[\"VLLM_API_URL\"], api_key=os.environ[\"VLLM_API_KEY\"]\n",
    "#     ),\n",
    "#     parameters=VLLMInferenceEngineParams(logprobs=True, temperature=0),\n",
    "# )\n",
    "# llm_component = VLLMInferenceEngine(\n",
    "#     model_name_or_path=\"Qwen/Qwen2.5-1.5B-Instruct\" #\"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "#     , # gated model , alternate test Qwen/Qwen2.5-1.5B-Instruct\n",
    "#     credentials=InferenceEngineCredentials(\n",
    "#         api_url=os.environ[\"VLLM_API_URL_LLM\"], api_key=os.environ[\"VLLM_API_KEY_LLM\"]\n",
    "#     ),\n",
    "#    \n",
    "#     parameters=VLLMInferenceEngineParams(\n",
    "#         logprobs=True, temperature=0, seed=42\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "#############\n",
    "\n",
    "# OLLAMA\n",
    "# To run ollama, ensure ollama is installed and then :\n",
    "# ollama run llama3:latest\n",
    "# ollama run granite3-guardian:8b\n",
    "\n",
    "guardian_judge = OllamaInferenceEngine(\n",
    "    model_name_or_path=\"granite3-guardian:8b\",\n",
    "    credentials=InferenceEngineCredentials(api_url=os.environ[\"OLLAMA_API_URL\"]),\n",
    "    parameters=OllamaInferenceEngineParams(\n",
    "        num_predict=1000, temperature=0, repeat_penalty=1, num_ctx=8192, logprobs=True, top_logprobs=10\n",
    "    ),\n",
    ")\n",
    "\n",
    "llm_component = OllamaInferenceEngine(\n",
    "    model_name_or_path=\"llama3:latest\",\n",
    "    credentials=InferenceEngineCredentials(api_url=os.environ[\"OLLAMA_API_URL\"]),\n",
    "    parameters=RITSInferenceEngineParams(\n",
    "        num_predict=1000, temperature=0, repeat_penalty=1, num_ctx=8192, logprobs=True, top_logprobs=10\n",
    "    ),\n",
    ")\n",
    "\n",
    "#############\n",
    "\n",
    "#RITS (IBM Internal Only, VPN required)\n",
    "\n",
    "# guardian_judge = RITSInferenceEngine(\n",
    "#     model_name_or_path=\"ibm-granite/granite-guardian-3.3-8b\",\n",
    "#     credentials={\n",
    "#         \"api_key\": os.environ[\"RITS_API_KEY\"],\n",
    "#         \"api_url\": os.environ[\"RITS_API_URL\"],\n",
    "#     },\n",
    "#     parameters=RITSInferenceEngineParams(\n",
    "#         logprobs=True, top_logprobs=10, temperature=0.0, seed=42\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# llm_component = RITSInferenceEngine(\n",
    "#     model_name_or_path=\"meta-llama/llama-3-3-70b-instruct\",\n",
    "#     credentials={\n",
    "#         \"api_key\": os.environ[\"RITS_API_KEY\"],\n",
    "#         \"api_url\": os.environ[\"RITS_API_URL\"],\n",
    "#     },\n",
    "#     parameters=RITSInferenceEngineParams(\n",
    "#         logprobs=True, top_logprobs=10, temperature=0.0, seed=42\n",
    "#     ),\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d419d3d-922e-4e10-aefe-3e8617e65e01",
   "metadata": {},
   "source": [
    "### Create and run the explanation generation pipeline\n",
    "\n",
    "The pipeline streamlines local and global explanation generation process. The Extractor executes the CLoVE algorithm and generates a set of local explanations, and Clusterer executes GloVE algorithm and merges the local explanations into a global one.\n",
    "\n",
    "Pass `lime=False` to pipeline creation step if no local word-based verification is done. Similarly, use `fr=False` if FactReasoner is not used to verify global explanations.\n",
    "\n",
    "The resulting local and global explanations are saved in the path folder passed to the pipeline.run() call.\n",
    "\n",
    "The execution logs can be found in the logs folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aee994e0-9e5d-4903-81af-22c78c240fb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_policy_rules(guardian_config, dataset: AbstractDataset,guardian_judge: InferenceEngine,llm_component: InferenceEngine,local_expl: Literal[\"LIME\", \"SHAP\"] = \"LIME\",results_path: Path = Path(\"results\")):\n",
    "    \"\"\"Generate the policy rules.\n",
    "\n",
    "    Args:\n",
    "        guardian_config (Dict): guardian config,\n",
    "        dataset (AbstractDataset): Dataset to be used for running the pipeline,\n",
    "        guardian_judge (InferenceEngine): An LLM inference engine instance of the Granite Guardian,\n",
    "        llm_component (InferenceEngine): An LLM inference engine instance for all steps of the policy distillation pipeline\n",
    "        local_expl (Literal[&quot;LIME&quot;, &quot;SHAP&quot;], optional): local explanation model -- only LIME and SHAP are supported. Defaults to \"LIME\".\n",
    "        results_path (Path, optional): Output directory path. Defaults to Path(\"results\").\n",
    "\n",
    "    Returns:\n",
    "        List: A list of policy rules\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an instance of the guardian model\n",
    "    guardian = Guardian(\n",
    "        inference_engine=guardian_judge,\n",
    "        config=guardian_config,\n",
    "    )\n",
    "\n",
    "    # local explanation model -- only LIME and SHAP are supported\n",
    "    if local_expl == \"LIME\":\n",
    "        local_explainer = LIME(\n",
    "            dataset.dataset_name, guardian_config[\"label_names\"], n_samples=100\n",
    "        )\n",
    "    elif local_expl == \"SHAP\":\n",
    "        local_explainer = SHAP(\n",
    "            dataset.dataset_name, guardian_config[\"label_names\"], n_samples=100\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Only LIME and SHAP are supported\")\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(\n",
    "        extractor=Extractor(\n",
    "            guardian,\n",
    "            llm_component,\n",
    "            guardian_config[\"criterion\"],\n",
    "            guardian_config[\"criterion_definition\"],\n",
    "            local_explainer,\n",
    "        ),\n",
    "        clusterer=Clusterer(\n",
    "            llm_component,\n",
    "            guardian_config[\"criterion_definition\"],\n",
    "            guardian_config[\"label_names\"],\n",
    "            n_iter=10,\n",
    "        ),\n",
    "        lime=True,\n",
    "        fr=True,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Run pipeline\n",
    "    expl = pipeline.run(dataset, results_path=results_path)\n",
    "    return expl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "103d2fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-04 12:54:23] INFO pipeline.py:50: Built pipeline.\n",
      "[2025-12-04 12:54:23] INFO pipeline.py:51: Using LIME = True\n",
      "[2025-12-04 12:54:23] INFO pipeline.py:52: Using FactReasoner = True\n",
      "[2025-12-04 12:54:23] INFO pipeline.py:69: Results directory for BeaverTails: [results/BeaverTails/local/local_expl.csv, results/BeaverTails/global/global_expl.pkl]\n",
      "[2025-12-04 12:54:23] INFO concept_extractor.py:51: Loaded concepts from results/BeaverTails/local/local_expl.csv\n",
      "/Users/ingevejs/Documents/workspace/ingelise/risk-atlas-nexus-demos/risk-policy-distillation/.venv-risk-policy-distillation/lib/python3.12/site-packages/risk_policy_distillation/pipeline/pipeline.py:143: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cds[\"Zipped\"] = cds.apply(\n",
      "[2025-12-04 12:54:23] INFO bipartite_graph.py:106: Loaded the following graph:\n",
      "\tLabels = dict_keys([0, 1]) Sizes = [7, 2] Number of edges = 2\n",
      "/Users/ingevejs/Documents/workspace/ingelise/risk-atlas-nexus-demos/risk-policy-distillation/.venv-risk-policy-distillation/lib/python3.12/site-packages/risk_policy_distillation/pipeline/pipeline.py:143: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cds[\"Zipped\"] = cds.apply(\n",
      "[2025-12-04 12:54:23] INFO bipartite_graph.py:106: Loaded the following graph:\n",
      "\tLabels = dict_keys([0, 1]) Sizes = [7, 2] Number of edges = 2\n",
      "[2025-12-04 12:54:23] INFO clusterer.py:188: Clustering...\n",
      "[2025-12-04 12:54:23] INFO clusterer.py:43: Clustering 7 instances\n",
      "[2025-12-04 12:54:23] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-04 12:54:23] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s]\n",
      "[2025-12-04 12:54:26] INFO clusterer.py:70: Cleaned up 2 clusters with 4 total concepts\n",
      "[2025-12-04 12:54:26] INFO clusterer.py:76: ['neutral and curious inquiry', 'neutral and curious inquiry']\n",
      "[2025-12-04 12:54:26] INFO bipartite_graph.py:119: \n",
      "\t\t\tMerging 2 nodes on 0 side.\n",
      "[2025-12-04 12:54:26] INFO bipartite_graph.py:134: \t\tAdded a node: id = label = 7, probability = neutral and curious inquiry, num of subnodes = 1.0\n",
      "[2025-12-04 12:54:26] INFO clusterer.py:76: ['reasonable and respectful explanation', 'reasonable and respectful explanation']\n",
      "[2025-12-04 12:54:26] INFO bipartite_graph.py:119: \n",
      "\t\t\tMerging 2 nodes on 0 side.\n",
      "[2025-12-04 12:54:26] INFO bipartite_graph.py:134: \t\tAdded a node: id = label = 8, probability = reasonable and respectful explanation, num of subnodes = 1.0\n",
      "[2025-12-04 12:54:26] INFO clusterer.py:43: Clustering 2 instances\n",
      "[2025-12-04 12:54:26] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-04 12:54:26] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.33it/s]\n",
      "[2025-12-04 12:54:28] INFO clusterer.py:70: Cleaned up 1 clusters with 2 total concepts\n",
      "[2025-12-04 12:54:28] INFO clusterer.py:76: ['potential harm to self-esteem', 'potential harm to self-esteem']\n",
      "[2025-12-04 12:54:28] INFO bipartite_graph.py:119: \n",
      "\t\t\tMerging 2 nodes on 1 side.\n",
      "[2025-12-04 12:54:28] INFO bipartite_graph.py:134: \t\tAdded a node: id = label = 2, probability = potential harm to self-esteem, num of subnodes = 1.0\n",
      "[2025-12-04 12:54:28] INFO clusterer.py:43: Clustering 5 instances\n",
      "[2025-12-04 12:54:28] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-04 12:54:28] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.53it/s]\n",
      "[2025-12-04 12:54:31] INFO clusterer.py:43: Clustering 5 instances\n",
      "[2025-12-04 12:54:31] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-04 12:54:31] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.88it/s]\n",
      "[2025-12-04 12:54:38] INFO clusterer.py:70: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-04 12:54:38] INFO clusterer.py:43: Clustering 1 instances\n",
      "[2025-12-04 12:54:38] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-04 12:54:38] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.09it/s]\n",
      "[2025-12-04 12:54:41] INFO clusterer.py:70: Cleaned up 1 clusters with 1 total concepts\n",
      "[2025-12-04 12:54:41] INFO clusterer.py:76: ['potential harm to self-esteem']\n",
      "[2025-12-04 12:54:41] INFO bipartite_graph.py:119: \n",
      "\t\t\tMerging 1 nodes on 1 side.\n",
      "[2025-12-04 12:54:41] INFO bipartite_graph.py:134: \t\tAdded a node: id = label = 3, probability = potential harm to self-esteem, num of subnodes = 1.0\n",
      "[2025-12-04 12:54:41] INFO clusterer.py:43: Clustering 1 instances\n",
      "[2025-12-04 12:54:41] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-04 12:54:41] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.91it/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 3379.78prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 30393.51prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 8256.50prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 41527.76prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 26886.56prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 20971.52prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 16912.52prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 37117.73prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 34379.54prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 19599.55prompts/s]\n",
      "[2025-12-04 12:55:15] INFO clusterer.py:43: Clustering 5 instances\n",
      "[2025-12-04 12:55:15] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-04 12:55:15] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.99it/s]\n",
      "[2025-12-04 12:55:18] INFO clusterer.py:70: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-04 12:55:18] INFO clusterer.py:43: Clustering 1 instances\n",
      "[2025-12-04 12:55:18] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-04 12:55:18] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.60it/s]\n",
      "[2025-12-04 12:55:21] INFO clusterer.py:70: Cleaned up 1 clusters with 1 total concepts\n",
      "[2025-12-04 12:55:21] INFO clusterer.py:76: ['potential harm to self-esteem']\n",
      "[2025-12-04 12:55:21] INFO bipartite_graph.py:119: \n",
      "\t\t\tMerging 1 nodes on 1 side.\n",
      "[2025-12-04 12:55:21] INFO bipartite_graph.py:134: \t\tAdded a node: id = label = 4, probability = potential harm to self-esteem, num of subnodes = 1.0\n",
      "[2025-12-04 12:55:21] INFO clusterer.py:43: Clustering 1 instances\n",
      "[2025-12-04 12:55:21] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-04 12:55:21] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.33it/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 36792.14prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 24966.10prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 20867.18prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 18641.35prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 24385.49prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 32263.88prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 33288.13prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 39945.75prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 32263.88prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 36792.14prompts/s]\n",
      "[2025-12-04 12:55:56] INFO clusterer.py:43: Clustering 5 instances\n",
      "[2025-12-04 12:55:56] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-04 12:55:56] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.32it/s]\n",
      "[2025-12-04 12:55:58] INFO clusterer.py:70: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-04 12:55:58] INFO clusterer.py:43: Clustering 1 instances\n",
      "[2025-12-04 12:55:58] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-04 12:55:58] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.90it/s]\n",
      "[2025-12-04 12:56:01] INFO clusterer.py:70: Cleaned up 1 clusters with 1 total concepts\n",
      "[2025-12-04 12:56:01] INFO clusterer.py:76: ['potential harm to self-esteem']\n",
      "[2025-12-04 12:56:01] INFO bipartite_graph.py:119: \n",
      "\t\t\tMerging 1 nodes on 1 side.\n",
      "[2025-12-04 12:56:01] INFO bipartite_graph.py:134: \t\tAdded a node: id = label = 5, probability = potential harm to self-esteem, num of subnodes = 1.0\n",
      "[2025-12-04 12:56:01] INFO clusterer.py:43: Clustering 1 instances\n",
      "[2025-12-04 12:56:01] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-04 12:56:01] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 52.44it/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 33288.13prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 39568.91prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 14027.77prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 26214.40prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 38479.85prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 3986.98prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 40329.85prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 38479.85prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 28532.68prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 24966.10prompts/s]\n",
      "[2025-12-04 12:56:36] INFO clusterer.py:43: Clustering 5 instances\n",
      "[2025-12-04 12:56:36] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-04 12:56:36] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.62it/s]\n",
      "[2025-12-04 12:56:38] INFO clusterer.py:70: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-04 12:56:38] INFO clusterer.py:43: Clustering 1 instances\n",
      "[2025-12-04 12:56:38] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-04 12:56:38] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.13it/s]\n",
      "[2025-12-04 12:56:41] INFO clusterer.py:70: Cleaned up 1 clusters with 1 total concepts\n",
      "[2025-12-04 12:56:41] INFO clusterer.py:76: ['potential harm to self-esteem']\n",
      "[2025-12-04 12:56:41] INFO bipartite_graph.py:119: \n",
      "\t\t\tMerging 1 nodes on 1 side.\n",
      "[2025-12-04 12:56:41] INFO bipartite_graph.py:134: \t\tAdded a node: id = label = 6, probability = potential harm to self-esteem, num of subnodes = 1.0\n",
      "[2025-12-04 12:56:41] INFO clusterer.py:43: Clustering 1 instances\n",
      "[2025-12-04 12:56:41] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-04 12:56:41] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.89it/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 28728.11prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 38130.04prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 16131.94prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 34379.54prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 41527.76prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 30174.85prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 32263.88prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 35544.95prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 16980.99prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 32263.88prompts/s]\n",
      "[2025-12-04 12:57:15] INFO clusterer.py:43: Clustering 5 instances\n",
      "[2025-12-04 12:57:15] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-04 12:57:15] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.04it/s]\n",
      "[2025-12-04 12:57:18] INFO clusterer.py:70: Cleaned up 0 clusters with 0 total concepts\n",
      "[2025-12-04 12:57:18] INFO clusterer.py:43: Clustering 1 instances\n",
      "[2025-12-04 12:57:18] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-04 12:57:18] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.05it/s]\n",
      "[2025-12-04 12:57:23] INFO clusterer.py:70: Cleaned up 1 clusters with 1 total concepts\n",
      "[2025-12-04 12:57:23] INFO clusterer.py:76: ['potential harm to self-esteem']\n",
      "[2025-12-04 12:57:23] INFO bipartite_graph.py:119: \n",
      "\t\t\tMerging 1 nodes on 1 side.\n",
      "[2025-12-04 12:57:23] INFO bipartite_graph.py:134: \t\tAdded a node: id = label = 7, probability = potential harm to self-esteem, num of subnodes = 1.0\n",
      "[2025-12-04 12:57:23] INFO clusterer.py:43: Clustering 1 instances\n",
      "[2025-12-04 12:57:23] INFO SentenceTransformer.py:219: Use pytorch device_name: mps\n",
      "[2025-12-04 12:57:23] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.82it/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 15650.39prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 19972.88prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 14716.86prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 29330.80prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 30174.85prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 33026.02prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 28728.11prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 15592.21prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 37449.14prompts/s]\n",
      "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 1/1 [00:00<00:00, 27962.03prompts/s]\n",
      "[2025-12-04 12:57:58] INFO global_expl.py:34: Loaded 2 rules\n",
      "[2025-12-04 12:57:58] INFO global_expl.py:134: Stored global explanation to results/BeaverTails/global/global_expl.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rules': [{'prediction': 0,\n",
       "   'if_clause': 'reasonable and respectful explanation',\n",
       "   'despite_clauses': 'none'},\n",
       "  {'prediction': 0,\n",
       "   'if_clause': 'neutral and curious inquiry',\n",
       "   'despite_clauses': 'none'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the rules\n",
    "expl = generate_policy_rules(\n",
    "    guardian_config, dataset, guardian_judge, llm_component, local_expl=\"LIME\"\n",
    ")\n",
    "\n",
    "#print the output\n",
    "expl.print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-risk-policy-distillation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
